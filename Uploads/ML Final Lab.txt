# For this lab, you need to import pandas, NumPy, Matplotlib and Seaborn.

# You also need to import the following from sklearn: 
#   metrics and svm; 
#   GaussianNB from naive_bayes; 
#   confusion_matrix, plot_confusion_matrix, classification_report from metrics
#   LogisticRegression from linear_model
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics import (confusion_matrix, plot_confusion_matrix, classification_report, precision_score)
from sklearn.model_selection import train_test_split

from sklearn import svm
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression

# Read in the dataset heart.csv using pandas
df = pd.read_csv('heart.csv');

# Read first 5 rows of the dataset
df.head()

# Rename the columns for better readability 
columns_names = {'cp':'chest_pain_type','trestbps':'resting_blood_pressure', 'exang':'exercise_ang', 'chol': 'serum_cholesterol','fbs': 'fasting_blood_sugar', 'thal': 'max_heart_rate',}
# Hint: Use pandas rename() method and input columns_names to it
df = df.rename(columns=columns_names)
df

# Display the datatypes for all the features 
df.info()

# Check for NaN values
df.isna().any()

# Check the number of missing values per feature
df.isna().sum()

# Print rows having NaN values
# This is a good step to analyze missing fields and decide the best approach to deal with them.
df[df.isna().sum(axis=1) > 0]

# Compute the mean of exercise_ang column
mean_exercise_ang = df['exercise_ang'].mean()
mean_exercise_ang

# Replace NaN entries in the column 'exercise_ang' by its mean
# Hint: use fillna() method
df = df.fillna(value={'exercise_ang': mean_exercise_ang})

# Use the head method to confirm the new entries in rows 11 and 20
df.head(21)

# Remove the rest of the NaN values (row 5)
# Hint: use dropna() method
df = df.dropna()

# Make sure no more NaN values (same method as used above to first check for NaN values)
df.isna().any()

# Change the data type of the target to int, then display several entries to confirm
# Hint: Use .loc method to locate the target column
df['target'] = df.loc[:,'target'].astype(int)

# View a few random rows of the dataset to confirm target is changed to int
df.sample(10)

# Plot the distribution of age feature in the dataset using Seaborn
# Hint, use Seaborn's distplot() method and specify a number of bins
sns.distplot(a=df['age'], bins=10, hist=True, kde=True, rug=True)

# Define variables: X is everything but target; y is target. 
X = df.drop(['target'], axis=1)
y = df['target']

# Display the first few X entries and compare to the cleaned dataset
display(X.head())
df.head()

# Display the first few y entries and compare to the cleaned dataset 
display(y.head())
df['target'].head()

# Split data to training and test sets with split 70-30
# Use random_state = 42
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Run Logistic Regression model 
logreg = LogisticRegression(solver='liblinear', max_iter=2000)
logreg.fit(X_train, y_train)
logreg_pred = logreg.predict(X_test) 

# Obtain and plot the confusion matrix
conf = confusion_matrix(y_test, logreg_pred)
print(conf)
disp = plot_confusion_matrix(logreg, X_test, y_test,
display_labels=["0","1"],                          
cmap=plt.cm.Blues)
disp.ax_.set_title("Confusion Matrix")
plt.show()

# Compute the accuracy score; your result should be approximately 0.835
accuracy = (conf[0,0] + conf[1,1])/(conf[0,0]+conf[0,1]+conf[1,0]+conf[1,1])
print("Accuracy:", accuracy * 100, "%\n\n")
logreg_s = logreg.score(X_test, y_test)
print("Accuracy:", int(logreg_s *100), "%\n\n")
# Obtain the classification report 
w = classification_report(y_test, logreg_pred, labels=[0,1], target_names=['0','1'])
print(w)
# Obtain the precision score
ps = precision_score(y_test, logreg_pred)
print(ps)

# Run SVM model and obtain the predictions
# Import svm model
from sklearn import svm
# Create a svm Classifier
svm_model = svm.SVC(kernel='linear') # Linear Kernel
# Train the model using the training sets
svm_model.fit(X_train, y_train)
# Predict the response for test dataset
y_pred = svm_model.predict(X_test)

# Obtain and plot the confusion matrix
conf_svm = confusion_matrix(y_test, y_pred)
print(conf_svm)
disp = plot_confusion_matrix(svm_model, X_test, y_test,
display_labels=["0","1"],                          
cmap=plt.cm.Blues)
disp.ax_.set_title("Confusion Matrix")
plt.show()

# Obtain the accuracy score
accuracy_svm = (conf_svm[0,0] + conf_svm[1,1])/(conf_svm[0,0]+conf_svm[0,1]+conf_svm[1,0]+conf_svm[1,1])
print("Accuracy:", accuracy_svm * 100, "%\n\n")
svm_s = svm_model.score(X_test, y_test)
print("Accuracy:", int(svm_s *100), "%\n\n")
# Obtain the classification report
w = classification_report(y_test, y_pred, labels=[0,1], target_names=['0','1'])
print(w)
# Obtain the Precision Score
ps = precision_score(y_test, y_pred)
print(ps)

# Run the Naive Bayes algorithm 
# Create a Gaussian Classifier
gnb_model = GaussianNB()
# Train the model using the training sets
gnb_model.fit(X_train, y_train)
# Predict the response for test dataset
y_pred = gnb_model.predict(X_test)

# Obtain and plot the confusion matrix
conf_gnb = confusion_matrix(y_test, y_pred)
print(conf_gnb)
disp = plot_confusion_matrix(gnb_model, X_test, y_test,
display_labels=["0","1"],                          
cmap=plt.cm.Blues)
disp.ax_.set_title("Confusion Matrix")
plt.show()

# Obtain the accuracy score
accuracy_gnb = (conf_gnb[0,0] + conf_gnb[1,1])/(conf_gnb[0,0]+conf_gnb[0,1]+conf_gnb[1,0]+conf_gnb[1,1])
print("Accuracy:", accuracy_gnb * 100, "%\n\n")
gnb_s = gnb_model.score(X_test, y_test)
print("Accuracy:", int(gnb_s *100), "%\n\n")
# Obtain the classification report
w = classification_report(y_test, y_pred, labels=[0,1], target_names=['0','1'])
print(w)
# Obtain the Precision Score
ps = precision_score(y_test, y_pred)
print(ps)